{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "interesting-chance",
   "metadata": {},
   "source": [
    "## 预处理传入W2V的语料\n",
    "- jieba分词\n",
    "- #去除停用词\n",
    "- 拿到了分词后的文件，在一般的NLP处理中，会需要去停用词。由于word2vec的算法依赖于上下文，而上下文有可能就是停词。因此对于word2vec，我们可以不用去停词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "medical-psychology",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T08:31:30.576704Z",
     "iopub.status.busy": "2021-03-24T08:31:30.576704Z",
     "iopub.status.idle": "2021-03-24T08:31:30.627263Z",
     "shell.execute_reply": "2021-03-24T08:31:30.626265Z",
     "shell.execute_reply.started": "2021-03-24T08:31:30.576704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5884"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "legal-cradle",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-26T02:21:58.083298Z",
     "iopub.status.busy": "2021-03-26T02:21:58.083298Z",
     "iopub.status.idle": "2021-03-26T02:22:02.454859Z",
     "shell.execute_reply": "2021-03-26T02:22:02.454859Z",
     "shell.execute_reply.started": "2021-03-26T02:21:58.083298Z"
    }
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "#分词\n",
    "def separate(sentence):   \n",
    "    sep_text = jieba.cut(sentence.strip())  \n",
    "    sep_text_str = ' '.join(sep_text)\n",
    "    return sep_text_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "municipal-kazakhstan",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-26T02:22:02.455992Z",
     "iopub.status.busy": "2021-03-26T02:22:02.455992Z",
     "iopub.status.idle": "2021-03-26T02:22:08.325687Z",
     "shell.execute_reply": "2021-03-26T02:22:08.324985Z",
     "shell.execute_reply.started": "2021-03-26T02:22:02.455992Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\Zedd\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.685 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "with open(r'C:\\Users\\Zedd\\Desktop\\yf_amazon_20w.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "    # data = data.replace('\\n','')  #去转行\n",
    "    \n",
    "separated_data = separate(data)\n",
    "\n",
    "with open(r'C:\\Users\\Zedd\\Desktop\\yf_amazon_processed.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(separated_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-palestine",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\Zedd\\Desktop\\yf_amazon_20w.txt', 'r', encoding='utf-8') as f:\n",
    "    line = f.readline()\n",
    "    lss = []\n",
    "    while line:\n",
    "        lss.append(line.strip())\n",
    "\n",
    "import pandas as pd\n",
    "import swifter\n",
    "dff = pd.DataFrame({'content': lss})\n",
    "dff['sep'] = dff.content.swifter.apply(separate)\n",
    "\n",
    "content_sep = '\\n'.join(i for i in dff['sep']) #合并一列中的文本\n",
    "\n",
    "with open(r'C:\\Users\\Zedd\\Desktop\\yf_amazon_processed.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(content_sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-product",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-moldova",
   "metadata": {},
   "source": [
    "## 用word2vector深度学习模型训练获得词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bound-fiber",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-26T02:28:17.788878Z",
     "iopub.status.busy": "2021-03-26T02:28:17.788878Z",
     "iopub.status.idle": "2021-03-26T02:28:20.423660Z",
     "shell.execute_reply": "2021-03-26T02:28:20.423660Z",
     "shell.execute_reply.started": "2021-03-26T02:28:17.788878Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging\n",
    "\n",
    "def train_w2v():   #训练word2vector\n",
    "    logging.basicConfig(format = '%(asctime)s : %(levelname)s : %(message)s', level = logging.INFO)   #配置日志文件\n",
    "    sentence = word2vec.Text8Corpus(r'C:\\Users\\Zedd\\Desktop\\yf_amazon_processed.txt')   #加载语料库\n",
    "    model = word2vec.Word2Vec(sentence, sg=1, size=100, window=5, min_count=1, negative=3, sample=0.001, hs=1, workers=40)   #训练skip-gram(sg=1)模型\n",
    "    model.save(r'./w2v_model')   #保存模型\n",
    "    # model = word2vec.Word2Vec.load(r'')   # 对应的加载方式\n",
    "    \n",
    "def get_w2v(wordlist):   #获取词向量\n",
    "    model = word2vec.Word2Vec.load(r'./w2v_model') #加载模型\n",
    "    vecs =[]\n",
    "    for word in wordlist:\n",
    "        word = word.replace('\\n','')\n",
    "        vecs.append(model[word])\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-edward",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T02:19:01.441856Z",
     "iopub.status.busy": "2021-03-24T02:19:01.440858Z",
     "iopub.status.idle": "2021-03-24T02:19:01.448836Z",
     "shell.execute_reply": "2021-03-24T02:19:01.447840Z",
     "shell.execute_reply.started": "2021-03-24T02:19:01.440858Z"
    }
   },
   "source": [
    "word2vec.Word2Vec()  paramters\n",
    "- sg=1是skip—gram算法，对低频词敏感，默认sg=0为CBOW算法\n",
    "- size是神经网络层数（词向量维度），值太大则会耗内存并使算法计算变慢，一般值取为100到200之间。\n",
    "- window是句子中当前词与目标词之间的最大距离，3表示在目标词前看3-b个词，后面看b个词（b在0-3之间随机）\n",
    "- min_count是对词进行过滤，频率小于min-count的单词则会被忽视，默认值为5。\n",
    "- negative和sample可根据训练结果进行微调，sample表示更高频率的词被随机下采样到所设置的阈值，默认值为1e-3,\n",
    "- negative: 如果>0,则会采用negativesamping，用于设置多少个noise words\n",
    "- hs=1表示层级softmax将会被使用，默认hs=0且negative不为0，则负采样将会被选择使用。即我们的word2vec两个解法的选择了，如果是0， 则是Negative Sampling，是1的话并且负采样个数negative大于0， 则是Hierarchical Softmax。默认是0即Negative Sampling。\n",
    "- workers是线程数，此参数只有在安装了Cpython后才有效，否则只能使用单核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2v() #内存不够，别运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_w2v(wordlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-nickname",
   "metadata": {},
   "source": [
    "## Word2vec模型使用\n",
    "- 找出某一个词向量最相近的词集合\n",
    "- 查看两个词向量的相近程度\n",
    "- 找出不同类的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "#找出某一个词向量最相近的词集合\n",
    "req_count = 5\n",
    "for key in model.wv.similar_by_word('沙瑞金'.decode('utf-8'), topn =100):\n",
    "    if len(key[0])==3:\n",
    "        req_count -= 1\n",
    "        print key[0], key[1]\n",
    "        if req_count == 0:\n",
    "            break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看两个词向量的相近程度\n",
    "print model.wv.similarity('沙瑞金'.decode('utf-8'), '高育良'.decode('utf-8'))\n",
    "print model.wv.similarity('李达康'.decode('utf-8'), '王大路'.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "#找出不同类的词\n",
    "print model.wv.doesnt_match(u\"沙瑞金 高育良 李达康 刘庆祝\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-ceiling",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-death",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-26T03:19:00.858515Z",
     "iopub.status.busy": "2021-03-26T03:19:00.858515Z",
     "iopub.status.idle": "2021-03-26T03:19:00.865571Z",
     "shell.execute_reply": "2021-03-26T03:19:00.864548Z",
     "shell.execute_reply.started": "2021-03-26T03:19:00.858515Z"
    }
   },
   "source": [
    "**<font size=4 color=blue>XGBoost多分类</font>**\n",
    "- objective='multi:softmax'\n",
    "- numclass=?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "exceptional-journey",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-26T03:20:23.600214Z",
     "iopub.status.busy": "2021-03-26T03:20:23.600214Z",
     "iopub.status.idle": "2021-03-26T03:20:24.858855Z",
     "shell.execute_reply": "2021-03-26T03:20:24.858855Z",
     "shell.execute_reply.started": "2021-03-26T03:20:23.600214Z"
    }
   },
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "\n",
    "db = pymysql.connect(host = 'localhost',user = 'root',password = '123456',db = 'zeddhzm',charset='utf8')   \n",
    "cur = db.cursor()   \n",
    "sql = \"SELECT * FROM jdcomment_320_cleansed_3_get_dummies\"\n",
    "cur.execute(sql)\n",
    "result = cur.fetchall()\n",
    "df = pd.DataFrame(list(result),columns=['ID','CONTENT','CREATIONTIME','USEFULVOTECOUNT','CONTENT_LENGTH','REPLYCOUNT','IMAGECOUNT','SCORE','plus','not_plus','time_delta','COMPLETENESS','subjectivity','whether_useful'])\n",
    "cur.close()\n",
    "db.close()\n",
    "\n",
    "dff = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-front",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-tradition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from xgboost import plot_importance\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x = dff[['CONTENT_LENGTH','REPLYCOUNT','IMAGECOUNT','SCORE','plus','not_plus','time_delta','COMPLETENESS','subjectivity']]\n",
    "all_y = dff['whether_useful']\n",
    "x_train,x_test,y_train,y_test = train_test_split(all_x, all_y, train_size=0.7, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-sociology",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_0 = XGBClassifier(random_state=30)\n",
    "xgb_0.fit(x_train, y_train, eval_metric='auc')\n",
    "y_pred = xgb_0.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-recycling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-infection",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print ('Accuracy: %.5f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "print('Precision: ', precision_score(y_test, y_pred, labels=None, pos_label=0, average='binary'))\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "print('Recall: ', recall_score(y_test, y_pred, labels=None, pos_label=0, average='binary', sample_weight=None))\n",
    "\n",
    "from sklearn.metrics import f1_score \n",
    "print('F1 score: ', f1_score(y_test, y_pred, labels=None, pos_label=0, average='binary', sample_weight=None))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score        \n",
    "print('roc_auc_score: ', roc_auc_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
